% !TeX document-id = {5530719d-34df-4dd8-b4b5-e6ed092c2b36}
% !TeX program = pdflatex
% !BIB program = biber
\documentclass[sigconf,natbib=false]{acmart}

\usepackage{amsmath}
\let\openbox\relax
\usepackage{amsthm}
\newcommand\bmmax{2}
\usepackage{bm}

\usepackage[ruled,vlined]{algorithm2e}

% Get Booktab style ruling in algorithm blocks.
% Values shamelessly taken from egreg's answer in 
% https://tex.stackexchange.com/a/345745/82917
\makeatletter
\renewcommand*{\@algocf@pre@ruled}{\hrule height\heavyrulewidth depth0pt \kern\belowrulesep}
\renewcommand*{\algocf@caption@ruled}{\box\algocf@capbox\kern\aboverulesep\hrule height\lightrulewidth\kern\belowrulesep}
\renewcommand*{\@algocf@post@ruled}{\kern\aboverulesep\hrule height\heavyrulewidth\relax}
\makeatother

% Imports Ahoy

%\usepackage[dvipsnames]{xcolor}
%\usepackage{graphicx}
\usepackage{tikz}
\usepackage{varwidth}
\usetikzlibrary{arrows.meta, calc, fit, positioning}

\usepackage[title]{appendix}

\usepackage{etoolbox}
\usepackage[binary-units, per-mode=symbol]{siunitx}
\robustify\bfseries
\robustify\emph
%\robustify\uline
\sisetup{detect-all, range-phrase=--, range-units=single, detect-weight=true,detect-inline-weight=math, table-format=1.3}
\DeclareSIUnit{\packet}{p}

\makeatletter
\let\MYcaption\@makecaption
\makeatother

\usepackage[font=footnotesize]{subcaption}
\usepackage{awesomebox}

\makeatletter
\let\@makecaption\MYcaption
\makeatother

%\usepackage[basic]{complexity}
\usepackage[super,negative]{nth}

\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage{pifont}

\usepackage{booktabs}
%\usepackage[
%activate={true,nocompatibility},
%final,
%tracking=true,
%kerning=true,spacing=true
%]{microtype}
%\microtypecontext{spacing=nonfrench}

\usepackage[maxnames=2,maxbibnames=99,mincrossrefs=99,sortcites,style=numeric-comp
%,backend=biber
]{biblatex}
\addbibresource{papers-off.bib}
\addbibresource{confs-off.bib}
\addbibresource{books-off.bib}
\addbibresource{rfc.bib}
\addbibresource{misc.bib}

%\DeclareFieldFormat[inproceedings]{doi}{}
%\DeclareFieldFormat[article]{doi}{}
%\DeclareFieldFormat*{url}{}
%\DeclareFieldFormat[online]{url}{\mkbibacro{URL}\addcolon\space\url{#1}}
%\DeclareFieldFormat[report]{url}{\mkbibacro{URL}\addcolon\space\url{#1}}

%picky abt et al.
\usepackage{xpatch}

\usepackage{url}
\usepackage{hyperref}
\usepackage[nameinlink]{cleveref}
\newcommand{\crefrangeconjunction}{--}
\crefname{table}{table}{tables}

\DefineBibliographyStrings{english}{%
	andothers = {\emph{et al}\adddot}
}
\DeclareFieldFormat[inproceedings]{url}{}
\DeclareFieldFormat[article]{url}{}
%\DeclareFieldFormat[inproceedings]{doi}{}
%\DeclareFieldFormat[article]{doi}{}
%\DeclareFieldFormat[inproceedings]{editor}{}
%\DeclareFieldFormat[proceedings]{editor}{}
%\DeclareFieldFormat[article]{editor}{}

\newcommand{\fakepara}[1]{\noindent\textbf{#1:}}

% Official colours!

\definecolor{uofguniversityblue}{rgb}{0, 0.219608, 0.396078}

\definecolor{uofgheather}{rgb}{0.356863, 0.32549, 0.490196}
\definecolor{uofgaquamarine}{rgb}{0.603922, 0.72549, 0.678431}
\definecolor{uofgslate}{rgb}{0.309804, 0.34902, 0.380392}
\definecolor{uofgrose}{rgb}{0.823529, 0.470588, 0.709804}
\definecolor{uofgmocha}{rgb}{0.709804, 0.564706, 0.47451}

\definecolor{uofglawn}{rgb}{0.517647, 0.741176, 0}
\definecolor{uofgcobalt}{rgb}{0, 0.615686, 0.92549}
\definecolor{uofgturquoise}{rgb}{0, 0.709804, 0.819608}
\definecolor{uofgsunshine}{rgb}{1.0, 0.862745, 0.211765}
\definecolor{uofgpumpkin}{rgb}{1.0, 0.72549, 0.282353}
\definecolor{uofgthistle}{rgb}{0.584314, 0.070588, 0.447059}
\definecolor{uofgpillarbox}{rgb}{0.701961, 0.047059, 0}
\definecolor{uofglavendar}{rgb}{0.356863, 0.301961, 0.580392}

\definecolor{uofgsandstone}{rgb}{0.321569, 0.278431, 0.231373}
\definecolor{uofgforest}{rgb}{0, 0.317647, 0.2}
\definecolor{uofgburgundy}{rgb}{0.490196, 0.133333, 0.223529}
\definecolor{uofgrust}{rgb}{0.603922, 0.227451, 0.023529}

% End Imports

%\usepackage[english]{babel}
\usepackage{blindtext}

% Copyright
%\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference info
%\setcopyright{none}
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

%\settopmatter{printacmref=false, printccs=false, printfolios=true}
\settopmatter{printfolios=true}

% DOI
\acmDOI{12345}

% ISBN
\acmISBN{67890}

%Conference
\acmConference[SOSR '21]{Symposium of SDx Research 2021}{September \numrange{20}{21}, 2021}{Online}
\acmYear{2021}
%\copyrightyear{}

%% {} with no args suppresses printing of the price
\acmPrice{15.00}

\newcommand{\approach}{On Path Learning}
\newcommand{\approachshort}{OPaL}
\newcommand{\Coopfw}{\emph{CoOp}}
\newcommand{\coopfw}{\Coopfw}
\newcommand{\Indfw}{\emph{Ind}}
\newcommand{\indfw}{\Indfw}
\newcommand{\inring}{\textsc{In}}
\newcommand{\outring}{\textsc{Out}}

\makeatletter\let\expandableinput\@@input\makeatother
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% make math easy
\newcommand{\acval}[3]{\ensuremath{\operatorname{\hat{q}}(#1, #2, #3)}}
\newcommand{\acvalblank}{\ensuremath{\operatorname{\hat{q}}(\cdot)}}
\newcommand{\wvec}[1]{\ensuremath{\bm{w}_{#1}}}

\newcounter{insightc}
\newenvironment{insight}
	{
		\begin{tipblock}\refstepcounter{insightc}\textbf{Insight \theinsightc:}\em
	}
	{
		\end{tipblock}
	}

\begin{document}
	\title{Revisiting~the~Classics: Online~RL~in~the~Programmable~Dataplane}
	
%	\titlenote{Produces the permission block, and copyright information}
	%\subtitle{Extended Abstract}
	
%	\author{Paper \# XXX, XXX pages}
	 \author{Kyle A. Simpson}
	 \orcid{0000-0001-8068-9909}
	 \affiliation{%
	   \institution{University of Glasgow}
	   \city{Glasgow} 
	   \country{Scotland}
	 }
	 \email{k.simpson.1@research.gla.ac.uk}
	 \author{Dimitrios P. Pezaros}
\orcid{0000-0003-0939-378X}
	 \affiliation{%
	\institution{University of Glasgow}
	\city{Glasgow} 
	\country{Scotland}
}
\email{Dimitrios.Pezaros@gla.ac.uk}
	
% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{Simpson \emph{et al}.}
	
\begin{abstract}
?? In-NIC execution allows clever use of resources to bring 99.99th percentile action compute latency  from \SI{725}{\micro\second} to \SI{44}{\micro\second} (1.5 orders of magnitude!) in real-world policies for DDoS detection/prevention control.

?? We are the first to bring fully online RL to the dataplane!

?? Emphasise here and elsewhere: scales with core count, but with some designs (\Indfw) achieve better latency and per-core throughput with just one core/worker!
\end{abstract}

\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10003033.10003068.10003069</concept_id>
	<concept_desc>Networks~Data path algorithms</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10003033.10003099.10003102</concept_id>
	<concept_desc>Networks~Programmable networks</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10003752.10010070.10010071.10010261</concept_id>
	<concept_desc>Theory of computation~Reinforcement learning</concept_desc>
	<concept_significance>300</concept_significance>
	</concept>
	<concept>
	<concept_id>10010147.10010257.10010258.10010261</concept_id>
	<concept_desc>Computing methodologies~Reinforcement learning</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Networks~Data path algorithms}
\ccsdesc[500]{Networks~Programmable networks}
\ccsdesc[500]{Computing methodologies~Reinforcement learning}

%\keywords{ACM proceedings, \LaTeX, text tagging}
	
\maketitle

\setlength{\aweboxleftmargin}{0.12\linewidth}
\setlength{\aweboxcontentwidth}{1.97\linewidth}
	
\section{Introduction}
?? REDO ME: this is an old abstract

?? Make a point of explaining why in-NIC... in general!

\begin{figure}
	\centering
	\includegraphics[keepaspectratio, width=0.9\linewidth]{figures/arch-with-p4}
	\caption{\approachshort{} brings low-latency, online reinforcement learning directly to the dataplane. SoC- and NetFPGA-based SmartNIC devices expose spare compute---making in-situ, asynchronous processing and learning possible alongside P4 dataplanes. Classical RL policy methods are the key to making this computationally feasible.\label{fig:netro-arch}}
\end{figure}

Automatic optimisation, control, and defence of networks are at last becoming commonplace. Data-driven networking has led the charge in traffic optimisation, congestion control and packet classification via adaptive techniques such as Reinforcement Learning (RL), where every change and its measured effects further improve future decisions. Considering that the network evolves in its use and deployed protocols, this flexibility is essential. Yet data-driven methods suffer from a key weakness: they are dependent on both the recency and accuracy of input state. An out-of-date view of the world will lead to suboptimal choices, as will long processing times. These can result in worse performance and slower adaptation to the evolving network.

Programmable dataplanes and in-switch compute, then, hold promise for integrating these new techniques in a feasible and efficient manner (beyond dedicated servers or virtualised network functions). For RL, key tasks include policy evaluation, online training, state collection, and action execution -- each of these introduces some degree of sensitivity to state accuracy. Ideally then, all logic would run on these programmable devices. Yet, there is often a finite budget in microcode/FPGA space, per-packet processing times, and available cores for execution. Moreover, necessary hardware, such as floating-point units, is unavailable in almost all cases.

The precise costs and trade-offs which operators and designers must make have yet to be identified. This follows from a design space explosion induced by many necessary workarounds. For instance, quantisation or fixed-point arithmetic will allow training and control on all devices, but introduces further questions: what degree of quantisation is most appropriate? What effect would this have on training accuracy, communications cost, or storage requirements? More concerns arise when we consider core allocation, local vs. distributed training, and reliable lightweight communication in multi-agent scenarios. In all cases, network operators will not consider tools which affect underlying traffic.

I aim to examine the effects of these choices on an existing RL-based DDoS attack mitigation system. To protect legitimate traffic, this controls packet drop and filtering for each flow using individual metrics observed by RL agents at ingress routers, and load measurements from several points along the path taken by the flow in question. The examined metrics include the throughput and latency of its individual components alongside system metrics: flow arrival-to-judgement time, and knowledge propagation time. Beyond this, it's crucial to identify what we can implement on pure P4-capable hardware. While extensions to P4 are fairly common in commodity hardware, the need for full design access (as in NetFPGA) or Micro-C (as in Netronome SmartNICs) represent a break from the clean, loop-free semantics of P4. As the market matures, these may represent different feature classes and price points.

I will discuss early development efforts (including challenges and results) on the Netronome Agilio SmartNIC, which supports the P4 language as well as some degree of arbitrary microcode. I intend to present how reinforcement learning execution and network telemetry will differ in the above metrics when compared to a vNF-based deployment (i.e., software on external commodity servers).

This paper contributes:
\begin{itemize}
	\item An analysis of why in-NIC RL is best-placed to interact with the network, and how classical RL methods and quantisation ensure computational feasibility (\cref{sec:motivation}),
	\item \emph{\approachshort{}}: a general-purpose in-NIC RL agent which scales with allocated device resources to meet latency or throughput demands (\cref{sec:design}),
	\item Potential integrations of \approachshort{} with state-of-the-art PDP applications to perform fully in-NIC, automated DDoS mitigation and ?? OTHER THING (\cref{sec:potential-integrations}),
	\item In-depth evaluation of how our approach affects carried dataplane traffic, performs under different policy sizes, and improves upon explicit offloading to produce a \SI{11.7}{$\times$} latency reduction compared to commodity hardware (\SI{16.4}{$\times$} at 99.99\nthscript{th} tail) (\cref{sec:evaluation}).
\end{itemize}

\section{Background and Motivation}\label{sec:motivation}
We discuss recent trends in programmable switch hardware
By combining this with insights from the ML/RL communities (past and present), we discuss why in-NIC RL is best-placed to interact with the network, and how classical RL methods and quantisation ensure computational feasibility.

Crucially, we train our focus on devices with a low port density, such as SmartNICs and NetFPGA devices.
The designs of these devices make it reasonable to move policy processing \emph{outside of the packet pipeline}; SmartNICs expose many additional programmable cores, and NetFPGAs allow for the synthesis of independent functional units.
The main benefit of doing so is that core control logic can be moved as close to the device as possible \emph{without impacting packet processing rates}.

?? TODO: trim down. Too ``thesis-y'', rather than ``conference-y''.
?? Save this elsewhere 'til then?

\subsection{Programmable hardware capabilities}
The introduction of the P4~\parencite{DBLP:journals/ccr/BosshartDGIMRSTVVW14} language has led to explosive growth in the research community surrounding in-network computation and offloading.
Providing a single language which is compatible with network devices of many form factors has been instrumental in the development of novel fine-grained traffic measurement approaches~\parencite{DBLP:conf/sigcomm/GuptaHCFRW18,DBLP:conf/sigcomm/ChenFKRR18,DBLP:conf/sosr/GhasemiBR17}.
However, this requires that there be a reasonably consistent interface and behaviour between device classes.
This need for consistency underpins the \emph{Programmable Switch Architecture} (PSA)~\parencite{p4-psa}, which defines a conceptual model of match-action tables divided into ingress and egress pipelines.
The PSA presents a sensible lower bound on the device capabilities required to implement a P4 dataplane, but the reality is more complex and interesting.

We would be remiss to believe that P4 and the PSA define all capabilities that programmable hardware supports.
Many compatible devices have legacies long predating these developments. 
For instance, many-core SOC-based Netronome SmartNIC~\parencite{netronome-smartnic}, and NetFPGA SUME~\parencite{DBLP:journals/micro/ZilbermanACM14} NICs allow virtually arbitrary programs to be specified and executed.
The first of these compiles from P4$\rightarrow$Micro-C$\rightarrow$bytecode (with Micro-C externs), while the latter can combine the P4$\rightarrow$NetFPGA toolchain~\parencite{DBLP:conf/fpga/IbanezBMZ19} with arbitrary circuit externs---both exposing further device capabilities beyond the specification.
This pattern extends to other SmartNICs, such as NVIDIA BlueField~\parencite{nvidia-bluefield} and Xilinx Alveo~\parencite{xilinx-alveo} devices.

Currently, low port density devices like the above are most likely to have specific general-purpose compute, as they are designed to suit high-performance offloading and middlebox development.
Even Intel Tofino~\parencite{barefoot-intel} ASICs, which architecturally mirror the PSA, expose additional matching capabilities and \emph{arithmetic logic unit} (ALU) functions via the Tofino Native Architecture.
Regardless, floating-point operations key to ML/RL workloads are very rarely supported outside of deep learning accelerators (which, in turn, lack packet switching functionality).
Moreover, the limited memory/block RAM and per-packet timing constraints endemic to these devices make this class of in-NIC offloading challenging.

\begin{insight}
	Current low-port density programmable network devices often have spare resources and extra capabilities beyond the PSA specification which can aid asynchronous, local compute.
\end{insight}

\fakepara{To-be-organised}
Interesting thing I realised from \textcite{DBLP:conf/sigcomm/NeugebauerAZAL018}: GPU and NIC might cause IOMMU contention? Making line-rate offload of DDN to GPU impractical. Unsure if I want to twin this with later measurements/plots.

\subsection{Reinforcement Learning}
\emph{Reinforcement learning} (RL) algorithms are methods of training an \emph{agent} to choose an optimal sequence of actions in pursuit of a given task \cite{RL2E}.
Like most machine learning methods, an RL algorithm uses gradient information to update the parameters used to approximate a function.
In RL contexts, this is a state$\rightarrow$action function known as a \emph{policy}.
When training, agents are given \emph{reward measurements} and a learned policy acts to maximise the \emph{expected discounted reward} received.
When a pre-trained policy is deployed, this signal is not required.

It's useful to consider how these algorithms differ from other ML use cases, such as classifiers.
The main differences lie in how this gradient information is used, and combined with \emph{reward measurements} received from the environment.
Rather than adapting the learned parameters along the gradient using an error value from a target value, because the optimal actions aren't known they adjust values using a \emph{Markov Decision Process} (MDP)---capturing state trajectories to adjust value based on past and future decisions.

Consider the single-step, semi-gradient Sarsa algorithm~\cite[pp. \numrange{217}{221}]{RL2E}:
\begin{subequations}
	\begin{gather}
		\delta_t = R_{t+1} + \gamma \acval{S_{t+1}}{A_{t+1}}{\wvec{t}} - \acval{S_t}{A_t}{\wvec{t}},\\
		\bm{w}_{t+1} = \bm{w}_{t} + \alpha \delta_t \nabla{\acval{S_t}{A_t}{\wvec{t}}},
	\end{gather}%
	\label{eqn:sg-sarsa}%
	where $\delta_t$ is known as the \emph{temporal-difference} (TD) error, $\acval{S}{A}{\wvec{}}$ denotes the \emph{value} of an action $A$ taken in state $S$ according to the policy $\wvec{}$, and the vector gradient $\nabla$ is taken with respect to $\wvec{}$. $\gamma,\alpha\in[0,1]$ are the discount factor and learning rate (governing the degree of forward planning and policy stability, respectively).
\end{subequations}
In essence, at each timestep the policy parameters ($\wvec{}$) are increased along the gradient ($\nabla{\acvalblank}$) using a fixed learning rate ($\alpha$) and a computed adjustment ($\delta_t$).
This adjustment is equal to the difference between the chosen action $A$'s value in state $S$ and the reward received ($R_{t+1} - \acval{S_t}{A_t}{\wvec{}}$), \emph{plus} some part of the \emph{next} action's value ($\gamma\acval{S_{t+1}}{A_{t+1}}{\wvec{}}$).
To give some context on the design space here, other algorithms may employ separate state value approximations, use the entirety of an agent's trace, or be tailored to characteristics of the policy approximator (e.g., how neural networks benefit from batching).

\begin{insight}
	Classical, single-step RL algorithms are computationally simple and are agnostic to the policy approximation used. Furthermore, they require only additions and multiplication to update and learn a policy online.
\end{insight}

\fakepara{RL in asynchronous environments}
There remains some degree of divergence between the theory and implementation of RL agents.
Consider \cref{fig:state-slip}: the traditional formulation of a Markov decision process assumes that an agent receives a new view of the world's state at fixed time intervals, and then decides upon and executes an action instantly.
The reality is that state information takes time to traverse the network, service times are offset by how quickly hosts respond to interrupts and deserialise requests, and action preference lists are often computed via expensive policy approximations.
Action installation also incurs costs in fields such as network administration, initially to contact the controller and then for those actions to be installed via the control plane.

These delays (and variance thereof) add noise to the state-action mapping being learned, which has a potent reduction to learning rate and final accuracy, even for simple grid world tasks according to \textcite{DBLP:journals/firai/TravnikMSP18}.
They in turn show that reordering algorithmic steps can reduce these costs for online 1-step algorithms, but that reducing this further requires detailed agent-environment co-design.
This principle has influenced the design of real network use cases, such RL-based congestion-control algorithms~\parencite{DBLP:journals/corr/abs-1910-04054}, showing that asynchrony is necessary for high-speed applications.
Achieving this often requires that state measurements are combined or coalesced~\parencite{DBLP:journals/corr/abs-1910-04054,DBLP:journals/tnsm/SimpsonRP20} while expensive computations are ongoing.
`Stopping the world' in the algorithmic sense causes significant performance degradation, as inference takes up to \SI{30}{\milli\second} in the above work on congestion control, or any time-sensitive control problems.

\begin{figure}
	\begin{subfigure}{0.45\linewidth}
		\resizebox{\linewidth}{!}{
			\centering
			\begin{tikzpicture}
				\node[circle, draw] (state) {$S$};
				\node[circle, draw] (state') at ($(state) + (0, -2)$) {$S'$};
				
				\node (agent) at ($(2, 0) + (state)$) {Agent};
				\node[below of=agent] (action) {Action $A$};
				
				\node[right of=state'] {$+ R$};
				
				\draw[->] (state) -- (state') node[midway, right] {$A$};
				
				\draw[dotted, ->, bend left = 30] (state) -- (agent);
				\draw[->] (agent) -- (action);
				\draw[dotted, ->] (action) -- (state);
		\end{tikzpicture}}
		\caption{Theory: state measurement, action computation, and learning are zero-cost.}
	\end{subfigure}
	\hspace{0.05\linewidth}
	\begin{subfigure}{0.45\linewidth}
		\resizebox{0.75\linewidth}{!}{
			\centering
			\begin{tikzpicture}
				\node[circle, draw] (state) {$S$};
				\node[circle, draw] (state') at ($(state) + (0, -1.5)$) {$S'$};
				\node[circle, draw] (state'') at ($(state') + (0, -1.5)$) {$S''$};
				
				\node (agent) at ($(2, 0) + (state)$) {Agent};
				\node[below of=agent] (action) {Action $A$};
				
				\node[right of=state''] {$+ R$};
				
				\draw[->] (state) -- (state') node[midway, right] {$\varnothing$};
				\draw[->] (state') -- (state'') node[midway, right] {$A$};
				
				\draw[dotted, ->, bend left = 30] (state) -- (agent) node[midway, above] {$t_1$};
				\draw[->] (agent) -- (action) node[midway, right] {$t_2$};
				\draw[dotted, ->] (action) -- (state') node[midway, below] {$t_3$};
		\end{tikzpicture}}
		\caption{Reality: costs of measurement and action lead to \emph{state drift}---over a time delay $t_1+t_2+t_3$, inaction transforms state $S$ into $S'$.}
	\end{subfigure}
	\caption{Asynchronous RL delays and state slippage (policy updates omitted).\label{fig:state-slip}}
\end{figure}

%?? Find some cites citing the relevance of this problem wrt. self-driving cars, robotics, etc.

The solution we propose is to make use of the recent wave of programmable network devices to \emph{bring reinforcement learning to the dataplane}---referring again to \cref{fig:state-slip}, we would place place state measurement ($t_1$), low-cost decision-making processes ($t_2$), and controlled systems ($t_3$) as close to one another as possible.
In networks, actions are most likely to be installed on backbone switches, bump-in-the-wire NICs or middleboxes, and in the NICs of end-hosts.
Ideally, these functions which comprise an RL agent would all be collocated on the same chip or device, but this is easier said than done. 
Both programmable devices and the network environment make this more difficult, as we'll examine in the sequel.

\begin{insight}
	Online control problems benefit from low-latency, local execution and training (i.e., in-NIC/switch).
\end{insight}

\subsection{Efficient policy approximation}
Some problems either evolve over time in unpredictable ways, or cannot be easily modelled.
This can then make \emph{online} learning of the task an attractive prospect, using a single available stream of experience.
\emph{Deep neural networks} (DNNs), particularly when used as the basis for an RL agent's policy, require vast amounts of experience to converge on an accurate parameter set.
In many problem domains, this equates to training from compute-years worth of distributed offline simulations, which is at odds with the need to adapt to changes in the underlying problem \emph{as they happen}.
Achieving stable learning requires sizeable batches of gradients to be computed, potentially using the entire experience replay from each simulation.
Although there has been a vested interest in efficiently \emph{executing} more complex function approximators such as DNNs in-NIC~\parencite{DBLP:journals/corr/abs-2002-08987,DBLP:journals/corr/abs-2009-02353,DBLP:conf/sigcomm/SanvitoSB18,DBLP:journals/corr/abs-1801-05731,langlet-ml-netronome}, the computational cost of gradient computation via backpropagation remains too significant on embedded hardware.

%?? Probably want some cites on the need for batching in NN methods, even though this is understood. DL book?

We then return again to what classical RL methods can offer us; in particular, the simple linear function approximation given by \emph{tile coding} \cite[pp.\ \numrange{217}{221}]{RL2E}.
The idea is simple: a policy is represented by sets of tiles, each covering one or more dimensions of the input state with several overlapping tilings (offset stepwise to provide generalisation).
A state vector produces a single `hit' in each tiling, which provides both the set of action preference lists to sum during inference \emph{and} the set of tiles to update at the next timestep.
Crucially, this internal representation ensures that there are no data dependencies between any tile hit calculations for an input state vector.
This has important benefits for parallelisation.
To select an action, we can produce a task for each tiling---retrieving the list of action values contributed by the tile activated by the input state.
An aggregate step sums up all action preference lists, and the action with the highest value is then selected.
Updating the policy (with a new $\delta_t$ value) has no aggregate step, and as tile indices map to disjoint memory regions no locks are required.

?? Algo block/diagram?

For instance, a policy with $m$ sets of tiles (each having its own set of input dimensions), each containing $n$ overlapping tilings then creates $m \times n$ separate tasks.\footnote{Retrieving each individual action's value can be considered a work item (giving $m \times n \times a$ tasks when there are $a$ actions). However, this requires the tile coding operation to repeated $a$ times per tile, which is likely extremely wasteful outside of extreme degrees of parallelism.}
As an example, a real-world policy size which we examine later (\cref{sec:experimental-setup}) with one bias tile and \num{16} sets containing \num{8} tilings creates \num{129} work items.

Computing a tile hit requires one division per dimension, which is an expensive addition to the ALU operations we require from a platform.
However, if the width of tiles are fixed at powers-of-two, then these may be replaced with right bitshifts.
As we shall see shortly, this aligns with our choice of numerical representation.

\begin{insight}
	Tile coded policies are embarrassingly parallel in both inference \emph{and} learning. Moreover, in some instances the process can be altered to require only bitshifts, additions, and multiplication.
\end{insight}

\subsection{Numerical representation for embedded ML}
A key feature universally lacking from the target environments is floating-point arithmetic support.
Luckily, the task of bringing machine learning models to resource-constrained environments without these capabilities is well-studied.
Quantisation and alternative data formats have been suggested to make ML inference feasible on resource- and power-limited platforms, work around hardware constraints, or compute faster and more efficiently.
Lower bit depths reduce memory footprints, and improve throughput in designs such as \emph{bfloat16}~\parencite{bfloat16-blog} in Google TPUs~\parencite{DBLP:journals/sigops/XieDMKVZT18}, \emph{hfp8}~\parencite{DBLP:conf/nips/SunCCWVSCZG19}, and other floating point formats~\parencite{DBLP:journals/corr/abs-2007-01530}.
In many cases, accuracy losses for doing so are negligible.
Much of this work goes further still towards integer~\parencite{tensorrt-8bit} or binarised~\parencite{DBLP:journals/corr/MiyashitaLM16,DBLP:conf/eccv/RastegariORF16,DBLP:journals/corr/KimS16,DBLP:conf/nips/HubaraCSEB16} representations, sacrificing dynamic range for simpler arithmetic operations.
The works mentions above use similar techniques to make inference tractable on network hardware, i.e., to bring neural networks to the dataplane~\parencite{DBLP:journals/corr/abs-2009-02353,DBLP:conf/sigcomm/SanvitoSB18,DBLP:journals/corr/abs-1801-05731}.

To make RL workloads feasible under such constraints, we propose the use of quantised, fixed-point representations such as $Qm.n$ (i.e., $m$\si{bit} signed integers with an $n$\si{bit} fractional part), which allow us to evaluate and update policies using only integer arithmetic.
This is not only essential in performing this work, but also serves as a mechanism for reducing the processing and memory costs of function approximation ($t_2$, \cref{fig:state-slip}).

When combined with the above P4-driven techniques for in-network flow/device measurement, we at last have the mechanisms to collocate the key processes of an RL agent.
Moreover, the base P4 dataplane can be used to simplify parsing logic and offer runtime control over which flows/packets are monitored, alongside other packet actions.
This again fits our goals of integrating RL techniques directly within bump-in-the-wire installations and at end-hosts.

\begin{insight}
	Fixed point quantisation allows us to perform numerical calculations using only integer arithmetic.
	Existing literature to-date supports the effectiveness and use of these techniques on embedded ML deployments.
\end{insight}

\subsection{Netronome Platform Fundamentals}
As we implement this work on Netronome SmartNICs, it is necessary to explain the basics of the NFP architecture.
NFP chips are \emph{system-on-a-chip} (SoC) devices, and achieve scalable packet processing through sheer parallelism.
Aside from an ARM management chip and application specific functional units, most of the chip is composed of \emph{microengines} (MEs), grouped into \emph{islands} of 4 or 12 MEs.
All 12-ME islands are used by a default P4 pipeline.
Each ME has \numrange{4}{8} \emph{contexts} (hardware threads) which share a code store and large register file, offering zero-cost context switches triggered by I/O.
Contexts and MEs may send one another numbered signals, and MEs have a small \emph{next-neighbour} register file for passing values in one direction to the next ME on the same island.
MEs run a proprietary instruction set, compiled to via a \emph{(Micro-)C} compiler.
Beyond registers, the platform implements an explicit memory hierarchy scaling in size, location, and access cost as below:
$$\text{LMEM (ME)} < \text{CLS (Island)} < \text{CTM} < \text{IMEM (Chip)} < \text{EMEM}$$

\subsection{Discussion}
%?? relate update/execute operations here? i.e., policy execute needs only ALU add (and whatever is needed for tile code), train needs add+mul+shr. or in RL section? Should that come first?

To achieve \emph{online in-NIC learning}, we return our focus to older \emph{classical} reinforcement learning methods.
In particular, we suggest tile-coding, with one-step temporal-difference learning algorithms such as Sarsa or Q-learning.
These choices have important benefits for in-NIC execution.
Simpler function approximators do not require batches of inputs to learn in a more stable way, negating the additional memory required to store experience replays.
Tile-coding in particular admits many optimisations, being an embarrassingly parallel problem.
Finally, the choice of single-step algorithms (as opposed to $n$-step or Monte Carlo methods) bounds the amount of per-trace state required for online learning to just the last state-action pair, safeguarding the limited memory of the target devices.

\section{Design and Implementation}\label{sec:design}
Based on the design principles, problems, and potential solutions outlined throughout \cref{sec:motivation}, we present our design for an in-NIC, task-independent, online reinforcement learning system---\emph{\approachshort{} (\approach)}.
At a high level, \approachshort{} is designed to use the auxiliary compute exposed by general SmartNIC devices to offer low-latency online learning, scaling according to available on-chip resources at build time.
As the allocation of cores/chip area is set ahead of time by a framework or system administrator, \approachshort{} (\Coopfw) agents enumerate themselves at runtime initialisation.

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\linewidth}
		\centering
		\includegraphics[keepaspectratio, width=0.9\linewidth]{figures/ind}
		\caption{\Indfw{} (offline throughput-optimal). \emph{Workers} independently pull commands from (and push state-action pairs to) the environment, locking access to the policy when updates must occur.\label{fig:single-and-parallel:single}}
	\end{subfigure}
	\hspace{0.04\linewidth}
	\begin{subfigure}{0.45\linewidth}
		\centering
		\includegraphics[keepaspectratio, width=0.9\linewidth]{figures/coop}
		\caption{\Coopfw{} (online-optimal). A single \emph{controller} delegates RL computation and updates to many \emph{minion} threads, who operate on independent subtasks.\label{fig:single-and-parallel:parallel}}
	\end{subfigure}
	\caption{\approachshort{}'s compute strategies scale to fit device capacity according to either latency or throughput needs.\label{fig:single-and-parallel}}
\end{figure*}

\Cref{fig:netro-arch,fig:single-and-parallel} outline our design and implementation on Netronome SmartNIC hardware in pursuit of this goal: unused device resources beyond the P4-PSA spec \emph{can and should be used} to drive asynchronous environmental control.
We open-source our firmware design and control programs for the benefit of the community.\footnote{\url{https://github.com/FelixMcFelix/pdp-rl-paper}---currently private...}
Moving beyond the Netronome platform, we describe how our architectural choices may be adapted and improved upon by more bespoke hardware or FPGA-based deployment.

\subsection{System Model}
\approachshort{} is a general, task-independent framework for in-network, online training and execution of \emph{any reinforcement learning agent design} using classical methods.
\approachshort{} is agnostic to the meaning of state vectors it receives as inputs and the actions it produces, which are employed by other functional units or the dataplane.
However, in-NIC/in-network execution specifically benefits packet-, flow-, and network-level learning, control, and optimisation tasks.

\approachshort{} runs on one or more cores of a SmartNIC to convert fixed-point state measurements from the environment into a stream of actions using a stored policy.
As an example, this might be to map flow state and performance measurements into a queueing priority for future packets from that flow, or to compute and apply a rate limit to preserve quality of service.
These dedicated cores are then responsible for processing requests, computing actions, and updating the underlying policy in real time.
Combined with reward measurements, this policy can then be updated or trained from scratch entirely on the NIC, acting as a fully online RL agent.
An input state vector \emph{always} induces an action, and if desired updates the policy using either an included reward or one retrieved from memory according to a (configurable) key placed alongside the state.
This allows for simultaneous control and learning over independent systems by the same agent (i.e., optimising several flows with their own reward measures).

%However, high-speed data networks impose inviolable per-packet deadlines.

To protect traffic throughput and allow for effective deployment in as many environments as possible, \approachshort{} places RL execution on-chip, \emph{but off the main packet path}, communicating and running parallel to the main P4 dataplane.
As shown in \cref{fig:netro-arch}, this asynchrony allows coexistence with P4 programs, and imposes minimal/no impact on carried traffic for both bump-in-the-wire deployments and at end-points.
For instance, in the default deployment of a P4 packet processing pipeline on Netronome NFP chips several of these cores go unused (as does spare area on an FPGA design), making this paradigm possible.

The main interaction model is that platform-specific IPC mechanisms (message rings) are used to \emph{push} configuration, state vectors, and reward measurements to the RL system.
These same mechanisms are used by other cores on the same device to \emph{pull} output actions from the RL system.
Both input and output can occur on any other core of the device, i.e., as part of P4 \texttt{extern} plugins or a dedicated flow state measurement subsystem, while the P4 control plane itself provides granular control over which flows are monitored or affected.

Runtime reconfiguration and interaction occur via the control and/or dataplane: the ease of use of the P4 pipeline's match-action tables and custom protocol parsers, combined with the dedicated input pipe to the NIC's controller (the host machine), allow these to be cleanly separated or combined as needed.
Bit depth of quantised measurements/preferences, maximum policy sizes, and parallelisation strategy may be configured at compile time.

\subsection{Action and Update Computation}
\approachshort{} applies the insights of \textcite{DBLP:journals/firai/TravnikMSP18} to minimise action latency; an action is computed, sent out into the environment, and only then is the underlying policy updated.
Using one of the below strategies chosen at compile time, a state vector is tile coded, converted into action probabilities, and an action is chosen according to current choice of $\epsilon$ (exploration parameter).
This is then written out to the environment as in \cref{sec:agent-environment-communication}.
If updates are enabled, \approachshort{} then checks an internal hashmap for a previous state-action pair matching the current instruction source, and if found then a policy update is performed.
Updates are computed using \emph{single-step semi-gradient Sarsa} (\cref{eqn:sg-sarsa}), though modification to support other single-step methods would be trivial.
The new state- or tiles-action pair is then written into storage.
\approachshort{} can be configured to automatically select values of the input state vector as keys for state and reward storage.

Two main firmware models govern how the computation-intensive parts of these tasks (tile-coding, action preference list construction, policy updates) are carried out:
\begin{description}
	\item[\Indfw{} (\cref{fig:single-and-parallel:single})] Separate threads listen for new states, and each performs its work sequentially. Computing an action list requires a \emph{read lock} on the policy. If an update occurs, the core requests a \emph{write lock} before updating, greatly limiting online throughput. \emph{Tile lists} are stored for update computation.
	\item[\Coopfw{} (\cref{fig:single-and-parallel:parallel,alg:parsa})] Threads cooperate on processing state vectors, minimising latency. Minions have a fixed list of work items, while a controller thread sends compute/update commands before awaiting worker completion. Work items are disjoint, requiring no policy locks. \emph{State vectors} are stored for update computation.
\end{description}
Each offers a different point of optimisation; if updates are disabled, then the \emph{\indfw{}} model can maximise throughput, while the \emph{\coopfw{}} model is designed to minimise decision latency and needs no locks for updating the policy (increasing \emph{online learning} throughput).
We note that our parallel, wait-free Sarsa implementation---\emph{ParSa} (\cref{alg:parsa})---is made possible only by the use of quantisation to integers, as atomic operations on floating point numbers typically do not exist outside of graphics- or tensor-processing hardware.

Latency and throughput, as in many networked systems, have different effects upon RL agents according to their design and target problem.
Higher throughput for an RL agent is a necessity for per-flow or per-packet applications, which can require high decision-per-second rates even after combining state measurements received from the environment, such as flow control in DDoS prevention.
Equally, lower latency affords an RL agent finer-grained control and learning of a problem, being able to react sooner to new information (e.g., device state in a routing optimisation problem, or queue depth when trying to enforce packet pacing).

We found that, paradoxically, it was more efficient in the \Coopfw{} case \emph{to do more work} by having each worker recompute its tile subset from a stored state.
It transpired that cacheing this data placed a larger \texttt{memcpy} in the serial section, whose size did not scale at all with bit depth.
Additionally, we do not use bitshifts in place of division operations in our implementation, due to the strict limits that power-of-two tile widths place upon policy design.

\subsection{Agent-Environment Communication}\label{sec:agent-environment-communication}
\approachshort{} uses \emph{multiple-producer/multiple-consumer} (MPMC) messaging channels to communicate with other computational elements; be they P4 programs on the packet path, or additional on-chip analysis and control modules.
Through these channels the system \emph{pushes} state vectors, reward measures, and control/setup packets as its inputs, and \emph{pulls} a stream of state-action pairs as its outputs.
This allows agent decisions to be made asynchronously---preventing packet stalling---and allowing for many parallel decision-making agents to be used if desired.
The key insight of this mechanism is that on-chip reward/state signals enjoy first-class support in much the same manner as packets from the P4 dataplane, allowing agents to act on environmental signals from other on-NIC/chip asynchronous processes or the controller.

Our implementation uses platform-specific IPC mechanisms (\emph{EMEM ring buffers}) with hardware signalling mechanisms on work arrival to achieve this.
Due to the lack of memory allocation in these environments, P4 pipeline threads request buffers for packet payloads using a shared freelist to enable state/configuration/policy data handover.
Packet headers are extracted and parsed using the tooling and data types autogenerated by the P4 pipeline.
We found that this costs a median \SIrange{126}{140}{\nano\second} communication time, comparable to message channels in the Rust and Go languages on commodity hardware.

\subsection{Intra-Agent Communication}
Even with embarrassingly parallel problems, optimising for latency requires meticulous care in how work is passed out and aggregated.
This is truer still when moving from the moderately fine-grained, efficient control of classical RL methods ($\sim$\SI{1}{\milli\second}) to its logical limit (tens of \si{\micro\second}).
Ordinarily, the serialisation/marshalling of requests, responses, and safe shared data access can incur significant extra overheads.
On-chip execution and the nature of action preference computation allow us to use lockless atomic aggregation, removing the overheads of explicit messaging/packetisation.
Moreover, physically adjacent functional units may have special-purpose shared registers or share a small fast cache to accelerate communication further.

Our implementation exploits the locality of threads, cores, and their parent islands in the NFP architecture.
Policy compute/update tasks and configuration updates are passed between cores using these direct \emph{next neighbour} registers, signalling all child threads in response.
Each task performs atomic adds to a shared preference list, and atomically increments an acknowledgement counter to be periodically checked by the master thread, implementing the wait-free \emph{ParSa} algorithm we introduce (\cref{alg:parsa}).

In earlier designs we had experimented with bounded buffers as in \cref{sec:agent-environment-communication} for this, modified to be located solely in CLS memory, dedicating the master thread to result aggregation.
We found that this created a performance bottleneck at this final stage, causing significant head-of-line blocking for each of the workers.
Similarly, our next neighbour work notification scheme ($\sim$\SI{20}{\nano\second} latency per relayed message) was examined against \emph{reflector} and \emph{work queue} IPC mechanisms (\SIlist{58;126}{\nano\second} per messaged core).
These slower messages can be used in theory to skip ahead into longer ME chains.


\begin{algorithm}
	\caption{ParSa---\emph{Par}allel \emph{Sa}rsa\label{alg:parsa}}
	\SetKwProg{generate}{Function \emph{generate}}{}{end}
	
	Map store=new Map(obj, queue)\;
	\generate{Object pivot}{
		\ForAll{child $c$ in pivot}{
			\If{ $c$'s FieldContext is not set and $c$ is fusible}{
				generate($c$)\;
			}
		}
		build pivot's fieldContext $fc$\;
		EmitClassName\;
		EmitFields($fc$)\;
		EmitMethods($fc$)\;
	}
\end{algorithm}

\subsection{Reconfigurability}
\approachshort{} allows for policy dimensions, size, action count, tiling strategies, and algorithmic learning parameters ($\alpha, \gamma, \epsilon$, related falloffs) to be chosen and changed at runtime using at most two control packets.
This extends to policy data, which may be imported from an offline pre-trained model via many such packets, and exported via the NFP binary support package on the host machine.
Some aspects must be chosen at compile time; bit depth, work allocation strategy, and maximum policy/tiling/state sizes---these govern core operation or allocated memory regions.

In our implementation, configuration packets are carried over UDP and signalled to the P4 parser using a reserved (pool 2~\parencite{rfc2474}) DSCP value as used by \textcite{DBLP:conf/isca/LiLYCSH19}.
While we primarily use this to automate parser generation, this also allows for configuration packets to be received from only trusted hosts (over the dataplane if needed) via P4 rules.
Our control packet generation library (and evaluation frameworks which build upon it) are written in Rust.

\subsection{Work Allocation}
Due to the lack of dynamic memory allocation and to simplify action-value lookups, policies cannot be effectively stored sparsely.
As a consequence, tiling space requirements scale exponentially with dimension count, and so higher-dimension tilings must be placed in memory regions large enough to allocate space for them.
As a result, tilings are split across memory regions, giving different access and compute costs to different tasks---ranging over CLS, CTM and IMEM in increasing latency.

We use a simple first-fit work placement algorithm run on the NFP, placing the largest work item into the least loaded context of the least loaded ME.
The cost of any individual work item (according to its dimension count and memory location) was empirically measured offline, and we weight the total cost per ME to account for the number of minion threads available.
This weighting specifically accounts for the presence of the controller thread on the first ME.
This work allocation and any cached per-work-item values are recomputed when any configuration is installed or changed.
Naturally, for $n$ tilings/tasks and $m$ threads this procedure is $\mathcal{O}{\left(n\log{m}\right)}$: two find/update min operations into binary heaps per task, storing $m/8$ and $\le8$ costs respectively.

\subsection{Variable Quantisation Bit Depth}
At compile-time, \approachshort{} can be configured to use \SIlist[list-final-separator = { or }]{32;16;8}{\bit} values in input states and its internal policies.
Naturally, smaller bit depths reduce the storage required for policies, tiling data, and stored state-action pairs---allowing more complex problems to be modelled using more dimensions or fine-grained policies.
Through the lens of computational efficiency, reduced bit depth should allows action preference lists to be read using fewer I/O operations (as more values may fit into a single machine word).

We had investigated bit-stuffing several such values into a single word for our atomic writeback mechanism (as the platform offers both \SIlist{32;64
}{\bit} atomic addition).
This is analogous to SIMD---through clever use of padding bits---but we found that manipulating tiles into the correct format added \SI{10}{\percent} extra overhead.
We investigate further performance implications of changing bit depth in the sequel.

%Potential: low-latency train for some specific points, and high throughput for others? Think on the exact note I took...
%
%``can have accel'd offline, train online subset w/ my approach''
%
%THis might mean "do 32-bit online", then downsample to 8-bit policy for high-throughput mode.
%
%Can changes in trained policy be used to transfer to a more complex function approximator elsewhere?

\subsection{Limitations}\label{sec:limitations}
Unfortunately, direct rule installation into P4 tables from the SmartNIC itself is not, in general, possible.
To achieve line-rate match-action table lookup, platforms like NFP use accelerated datastructures (e.g., DCFL~\parencite{DBLP:conf/infocom/TaylorT05}) which must be computed by the controller over the \emph{entire rule set}.
Even through externs, directly adding new rules is neither feasible nor safe.

We instead suggest on NFP chips that \texttt{extern}s or datapath stages which apply actions to packet processing should maintain a small local store of state-action pairs, and periodically send these back to the controller for batch rule installation.
This ensures that the vast majority of installed rules enjoy accelerated performance, while preventing rule installation delay on the newest decisions.
Platforms such as Intel Tofino greatly simplify this, with Tofino Native Architecture intrinsics such as \emph{Action Profiles/Selects} allowing a P4 action to be chosen based on a register value (i.e., an RL action).

As is expected of parallel algorithms, efficient division of work, communication, and aggregation add per-task overheads in both the serial and parallel portions.
Even in wait-free algorithms, this requires a minimum number of workers to improve upon the latency bounds of a serial approach.
We investigate the exact worker count requirements imposed to break even or improve upon single-threaded execution in the sequel.

\subsection{Targeting other device classes}
While this design caters primarily to Netronome's SmartNIC devices, many aspects of this design have clear analogues in NICs of similar form-factor, such as their close NetFPGA SUME cousins.
For instance, the availability of other cores can be handled by designing additional off-path functional units, much like the floating-point adders used by \textcite{DBLP:conf/isca/LiLYCSH19}.
Rather than general-purpose cross-core communication, an RL functional unit would be hard-wired to and from the custom actions that interface with it, using the P4$\rightarrow$NetFPGA toolchain to offer the base framework and granular control over packet and flow selection.
Further optimisations arise from this model: a NetFPGA design would be able to replicate and optimise further on this concept of dedicated, low-latency communication between functional units.
As our later results show, working on a bit depth below the native word size forces the compiler or programmer to emit excess code to extract and re-align preference values.
This reduces overall efficiency even though considerably fewer reads are made.
We expect that more bespoke hardware/FPGA co-design would allow this boiler-plate to be removed and enable the SIMD-like optimisations we discuss above without creative and costly bit-stuffing.

Bringing \approachshort{} to high-port density devices such as Intel Tofino-powered switches is more difficult.
As discussed in \cref{sec:motivation}, Tofino ASICs map closely to the P4 PSA, meaning that there are no spare asynchronous general-purpose compute units to place this work on.
Taking inspiration from real-time programming techniques, a potential solution is to divide RL actions across several received packets (i.e., iteratively computing a portion of the action preference list each time) until any further work would delay outbound transmission.
This, however, would introduce new issues surrounding concurrent accesses, work splitting, and altered timescales for learning: we leave their treatment and examination to future work.

\section{Evaluation}\label{sec:evaluation}
We investigate the performance of \approachshort{} as compared with classical RL techniques executed on commodity host machines, with \Coopfw{} offering a \SIrange{15}{21}{$\times$} speedup in median--\nth{99} state-action latency and \SI{9.9}{$\times$} greater online learning throughput.
Crucially, we find that in-NIC execution offers tight tail latency bounds compared with host-based approaches.
We report on how \approachshort{} scales to fit available device resources as cores/threads are added, noting that both \Indfw{} and \Coopfw{} designs outperform commodity hosts using just one core in latency and online throughput. 
Furthermore, \Indfw{} provides higher per-core offline throughput than host-based approaches, even though our measured hosts exhibit higher clock speeds.
Finally, we show that \approachshort{} has minimal impact on dataplane cross-traffic carried by its parent device.

\subsection{Experimental Setup}\label{sec:experimental-setup}
Testing machines had the following specifications, all having \SI{32}{\gibi\byte} RAM:
\begin{description}
	\item[\emph{MidServer}] Intel Xeon Bronze 3204 CPUs ($6\times$\SI{1.9}{\giga\hertz}),
	\item[\emph{HighServer}] Intel Xeon Silver 4208 CPU ($8\times$\SI{2.1}{\giga\hertz}),
	\item[\emph{Collector}] Intel Core i7-6700K ($4\times$\SI{4.2}{\giga\hertz}).
\end{description}
Evaluation of \approachshort{} was performed on server blades (\emph{Mid/HighServer}), each hosting a single Netronome Agilio LX 1x40GbE SmartNIC (NFP-6480, \SI{1.2}{\giga\hertz}).
These servers ran Ubuntu 18.04.5 LTS (4.15.0-140-generic).
We additionally use a more powerful consumer-grade machine (\emph{Collector}) for estimating host performance when offloaded to a network function, running Ubuntu 18.04.4 LTS (4.15.0-96-generic).
Control programs were built using rustc 1.52.1.

We run \approachshort{} on an island with \num{4} MEs of the NFP-6480 for a total of \num{32} hardware threads.
This is the maximum number of MEs on a single island which is not in use by an existing P4 pipeline.
Where feasible, we test \SIlist{32;16;8}{\bit} quantised arithmetic firmware versions.
All \approachshort{} timing measurements were repeated over \num{10000} trials (preceded by \num{1000} warmup packets), from which throughput was derived.
Host throughput measures were observed over \num{10} trials of \SI{10}{\second}.
Policy sizes are set to those matching a real-world DDoS control application~\parencite{DBLP:journals/tnsm/SimpsonRP20}: 20-dimension state vectors, a bias tile and 16 full tiling sets (7$\times$1-dim, 8$\times$2-dim, 1$\times$4-dim), 8 tilings per dimension set, 6 tiles per dimension, choosing between 10 actions.
For \Coopfw{} for instance, this creates \num{129} work items across \num{31} workers.
Although the model application's more successful \emph{SPF} agent design uses 3 actions, we choose a larger action set to investigate the performance of more complex agents.

\subsection{Experiments}

\fakepara{Raw inference and learning performance}
We compare how long it takes for \approachshort{} to compute actions and update its policy per state vector received, and report on the observed throughput of our designs.
We compare these against floating point (numpy-based) implementations running on a commodity host machine.
This allows us to demonstrate the performance differences between the \indfw{} and \coopfw{} configurations, particularly in how \indfw's required policy locks impact throughput.
We also report on the derived throughput measures accounting for the locking behaviour of each.
We compare online learning performance (input states produce an output, and then update the policy with appropriate locking) with offline (input states \emph{only} produce an output) in these cases.
State-action latency is a shared property of both cases, with the main difference being the impact on throughput of the update step.

Building on this, we vary the amount of allocated worker threads to show how \approachshort{} can scale to fit available compute resources on a device.
This also demonstrates the number of cores needed to achieve a given latency bound on a policy of representative complexity.
Moreover, to demonstrate how these costs vary as policy complexity increases, we vary the number of total dimensions included in the tiling.

\fakepara{Work allocation}
%?? Also our simple, ahead-of-time, work scheduler.
We verify that our heuristic, runtime work scheduler (\emph{Balanced}) makes meaningful use of the explicit memory hierarchy and cost of each work item.
We compare it versus several baselines: a \emph{Na\"{i}ve} chunked scheduler, a \emph{Random} baseline allocation, and a simple \emph{Modular} allocator (where a worker $j$ out of $w$ takes $k$ items given by $\mathit{work}_j=\left\{\left(j + i \times w\right) \bmod n_{\mathit{items}} \mid i \in [0,k) \right\}$).
We use maximum-size, \SI{32}{\bit} policies as described above.

\fakepara{End-to-end RL latency}
We compare the key RL decision-making latencies we discuss in \cref{fig:state-slip} ($t_1,t_2,t_3$) across 3 scenarios: completely in-NIC (\approachshort{}), offloading RL decisions to a SmartNIC's controller machine, and offloading to the controller of an OpenFlow-compatible switch (OvS).

\fakepara{Co-existence with the dataplane}
While varying the rate of full RL updates performed by \approachshort{}-\Coopfw{} (\SI{32}{\bit}) from \numrange{0}{16000} actions/s, we measure packet losses and sample latencies of cross traffic forwarded over a P4 pipeline hosted on our SmartNICs.
This allows us to quantify whether on-chip (out-of-path) execution impacts ordinary dataplane behaviour through indirect means: e.g., EMEM cache evictions or hidden resource contention.
%In particular, we stress test both decisions and policy updates of our \emph{Parallel} design at full throughput.

We perform these tests using Pktgen-DPDK~\cite{pktgen-dpdk}, placing an NFP in \emph{MidServer} as the device under test and connecting \emph{HighServer} over a \SI{40}{\giga\bit\per\second} DAC cable as the traffic source via the default firmware.
We perform throughput/loss tests using \num{7}/\num{1} transmit/receive queues at \SI{100}{\percent} send rate for 10 bursts of \SI{30}{\second}, and perform latency tests using \num{1}/\num{1} transmit/receive queue at \SI{10}{\percent} send rate for \num{200000} measurements.
DPDK was setup using four \SI{1}{\gibi\byte} hugepages.
Sent traffic was comprised of fixed-size \SIrange{64}{1518}{\byte} packets~\cite{rfc2544}.
CPU clock scaling was disabled on \emph{HighServer}.

\fakepara{Resource requirements}
Using the maximum policy size defined above, we investigate how the memory requirements imposed by \approachshort{} vary with the number of dedicated MEs, over and above a base P4 forwarding plane.
We report resource use for \SI{32}{\bit} \Indfw{} and \Coopfw{} agents with maximum policy sizes as above, with hash-tables sized to \num{4096} state-action pairs and \num{16} separate reward values.

%\fakepara{The impact of bit depth}
%We further investigate how bit depth affects the accuracy of policy by varying the number of bits used to represent the mantissa of action values and input state.

\fakepara{Deployability}
By timing agent setup and compile times, we describe the runtime costs needed for an administrator to repurpose an installed agent in a live network.

\subsection{Results and Discussion}\label{sec:results}
\fakepara{Raw inference and learning performance}
\Cref{tab:lats} shows how \approachshort{} compares in latency with a numpy-based (BLAS-accelerated) implementation of an RL policy\footnote{For brevity, we omit numpy-based integer results---summarised against a floating point implementation, median state-action latencies are \SI{14.6}{\percent} worse, with \SI{7.9}{\percent} longer update times.}.
We show that \Coopfw{} achieves sub-\SI{35}{\micro\second} median latency, with \nth{99} and \num{99.99}\nthscript{th} percentile latencies less than \SI{1}{\micro\second} worse using one island (4 MEs) of the NFP-6480.
This corresponds to \SIlist{15.0;21.0}{$\times$} speedups over a \emph{Collector} host.
Importantly, \Indfw{} achieves lower median state-action latencies (\SI{2.79}{$\times$}) \emph{and} update times (\SI{2.63}{$\times$}) than a dedicated \emph{Collector} host \emph{while requiring only a single core or dedicated functional unit}.

\begin{table}
	\caption{Latencies and computation times for \approachshort{} versus commodity hardware hosts. On-device execution is crucial in not only lowering latencies, but in reducing tail latencies. Lower is better, with the best marked \emph{in bold}.\label{tab:lats}}
	\resizebox{\linewidth}{!}{
		\expandableinput ../tables/build/latency-only32
	}
\end{table}

\begin{table}
	\caption{Action and update throughputs for \approachshort{} versus commodity hardware hosts. Most designs cannot scale online performance with additional cores. Higher is better, with the best marked \emph{in bold}.\label{tab:tputs}}
	\resizebox{\linewidth}{!}{
		\expandableinput ../tables/build/tput-only32
	}
\end{table}

\Cref{tab:tputs} compares \approachshort{}'s throughput against host-based execution.
We choose the worker count on host machines as the number of parallel agents that maximises throughput.
This equalled the amount of physical cores on each device, while moving beyond this (even below the number of hyper-threads) would hamper tail latencies by an order of magnitude.
To make the comparison fair in the context of many-core CPU environments, we include per-core throughput.
\Indfw{} achieves \SI{2.82}{$\times$} higher offline throughput than commodity \emph{Collector} hardware in spite of the NFP-6480 having a considerably slower clock speed (\SI{0.29}{$\times$}).
When compounded with the abundance of such weaker chips, in-NIC RL is able to deliver much higher throughput.
As anticipated, the \Coopfw{} strategy is key in achieving serviceable throughput in an online learning agent, \SI{9.9}{$\times$} that of a dedicated collector machine, due to the locking requirements mandating coordinated division of work.

By limiting the available workers in software, we show how \Coopfw{}'s policy update time (thus  online throughput---\cref{fig:vary-core}) and state-action latency (\cref{fig:vary-core-latency}) scale with available cores.
While the compute times and latency of \Coopfw{} always outperform the host-based floating point implementations, we observe that there are two distinct crossover points which must be met to overcome our own \Indfw{}; \num{8} workers for online throughput, and \num{3} workers for state-action latency.
Some artefacts of our environment and design choices are visible, such as the addition of new physical cores (MEs) being more significant than contexts, and the presence of some schedule bottlenecks.
Most importantly however, \approachshort{}-\Coopfw{}'s resource demand is tunable at compile time to meet the online training rate and/or action latency required by a task/environment.

\begin{figure}
	\resizebox{1.0\linewidth}{!}{
		\includegraphics{../plots/build/rl-perf-tester/vary-core}
	}
	\caption{
		\Coopfw{}'s online learning performance improves as additional cores are made available, on max size tasks (\num{129} work items). This requires \num{8} workers to offer greater online throughput than single-threaded in-NIC RL. Sharper performance increases occur when a new physical core is added (\numrange{7}{8}) or the scheduler works around a bottleneck (\numrange{13}{14}).\label{fig:vary-core}}
\end{figure}

\begin{figure}
	\resizebox{1.0\linewidth}{!}{
		\includegraphics{../plots/build/rl-perf-tester/vary-core-latency}
	}
	\caption{\Coopfw{}'s action latencies improve as additional cores are made available. This requires \num{3} workers (4 total contexts) to improve upon the state-action latency of single-threaded in-NIC RL.\label{fig:vary-core-latency}}
\end{figure}

\Cref{fig:vary-work,fig:vary-work-latency} show how policy complexity affects complete update costs and state-action latency respectively, scaling from a bias tile up to the full policy size used throughout\footnote{Input vectors here all have 20 elements regardless of the policy design.}.
\Coopfw{} always produces an action in less time than \Indfw{}, but requires at least one state-based tile to excel in online learning performance.
We note that this is a trivial case, in that the use of \emph{only} a bias tile negates the need for any input state (similar to a multi-arm bandit problem).

\begin{figure}
	\resizebox{1.0\linewidth}{!}{
		\includegraphics{../plots/build/rl-perf-tester/vary-work}
	}
	\caption{\Coopfw{} can fully process individual items faster than \Indfw{}---thus has higher online performance---on almost all policy sizes. Lower bit depths are more effective on simpler policies.\label{fig:vary-work}}
\end{figure}

We found that a key aspect of in-NIC execution is that it allows far tighter bounds on tail latency to be offered compared to host offloading.
Examining the state-action latencies in \cref{tab:lats}, we see that \num{99.99}\nthscript{th} percentile latencies exceed the median by \SIlist{0.58;0.66}{\percent} for \Indfw{} and \Coopfw{} respectively.
Similar results were observed for other bit depths.
By contrast, host-based tail latencies are at least \SI{40.53}{\percent} greater in our test setup when parallel workers are at or below the physical core count.
We show the cumulative distributions of these in detail in \cref{fig:lat-cumul}, noting how just one additional CPU-intensive task---potentially automated system updates, scans, or another vNF or traffic processing task---can impact tail latencies further (\emph{Float(Over)}).

\begin{figure}
	\resizebox{1.0\linewidth}{!}{
		\includegraphics{../plots/build/rl-perf-tester/vary-work-latency}
	}
	\caption{State-action latency scales with additional work in a similar manner to overall processing time; though \SI{32}{\bit} firmwares become more effective sooner (at \num{3} work items).\label{fig:vary-work-latency}}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}{0.45\linewidth}
		\resizebox{1.0\linewidth}{!}{
			\includegraphics{../plots/build/rl-perf-tester/latency-cumul}
		}
		\caption{\approachshort's \Coopfw{} design achieves consistent, tight latency bounds.}
	\end{subfigure}
	\begin{subfigure}{0.45\linewidth}
		\resizebox{1.0\linewidth}{!}{
			\includegraphics{../plots/build/rl-perf-tester/latency-cumul-broad}
		}
		\caption{Tail latencies suffer in host execution---particularly when oversubscribed.}
	\end{subfigure}
	\caption{Cumulative state-action latency plots for \approachshort{} and host-based execution.\label{fig:lat-cumul}}
\end{figure}

A somewhat remarkable trend is that the \SIlist{8;16}{\bit} versions of \approachshort{} consistently underperform compared to \SI{32}{\bit} designs, with the exception of smaller workloads (seen in the zoomed portions of \cref{fig:vary-work,fig:vary-work-latency}).
We found this to be rather surprising, given that our implementation is optimised to read and write policy tile data in batches (achieving fewer I/O operations as required).
Our understanding is that this is due to the native register width on the NFP being \SI{32}{\bit}, and so the compiler must emit extra instructions around arithmetic operations to correctly load and update values.
This matches \SI{32}{\bit} becoming dominant in more complex workloads: higher dimension tilings require more arithmetic operations to compute the hit tile index.
Most of the I/O comes after this step, causing ALU use to dominate.
This also explains why \SI{32}{\bit} becomes the best choice at different policy complexities for online (\cref{fig:vary-work}, 10 dims) and offline (\cref{fig:vary-work-latency}, 3 dims) \Coopfw{} agents, where hashtable accesses and a \texttt{memcpy} of the state vector fall into the serial portion of the algorithm in the online case.
Regardless, the \SIrange{2}{4}{$\times$} reduction in memory needed to store a policy holds.

\fakepara{Work allocation}
\Cref{fig:work-alloc-32} shows that our heuristic first-fit allocator (\emph{Balanced}) is key in achieving consistent sub-\SIlist{35;65}{\micro\second} latencies/processing times respectively.
The trend is repeated for all bit depths, noting the latency differences described above.
The constant difference between \emph{Action} and \emph{Update Prep} was observed to scale linearly with bit depth, matching storage and lookup work in the serial portion between parallel jobs (plots omitted).
The severe underperformance of the \emph{Na\"{i}ve} allocator confirms that work item complexity is correlated with its ID, as batching work in contiguous chunks leads to some workers receiving exclusively high-dimensionality tile sets.
The minor gap in performance lower bounds between the \emph{Random} and \emph{Balanced} allocators suggests that further optimisations can be made in this way.
We expect that closing or exceeding this gap may require more complex modelling of NFP's hardware thread interactions, which lies far beyond the scope of in-NIC scheduling.
Some additional complexity may be tolerated, subject to code store limits---scheduling runs exactly once per configuration change, so does not impact per-action code.

\begin{figure}
	\resizebox{1.0\linewidth}{!}{
		\includegraphics{../plots/build/rl-perf-tester/work-strat-32bit}
	}
	\caption{Action/update compute times in a \SI{32}{\bit} \Coopfw{} agent under different work schedulers.\label{fig:work-alloc-32}}
\end{figure}

\begin{figure}
	\resizebox{1.0\linewidth}{!}{
		\includegraphics{../plots/build/rl-perf-tester/vary-core-tput-on-per}
	}
	\caption{Throughput per added worker in a \Coopfw{} agent.\label{fig:tput-per-core}}
\end{figure}

An interesting aspect of \Coopfw{} and the \emph{ParSa} algorithm in general is that adding additional cores has both diminishing returns and key thresholds to pass.
Consider \cref{fig:tput-per-core}, where the throughput per worker decreases over time but occasionally increases sharply.
Later downward ticks (\numrange{25}{29} workers) correspond to plateaus in throughput.
This is a common problem stemming from the granularity of work items, where the heuristic scheduler cannot find a better solution to a bottleneck until extra cores are allocated.
We measured individual work items in state-action computation to take a mean \SIlist{5.2; 6.2; 9.7; 11.0}{\micro\second} for bias, CLS, CTM and IMEM tilings respectively.
While there is still a \SI{4.2}{$\times$} factor of task oversubscription at maximum worker count, it is clear that latencies are bound below by the length of the longest task.

\fakepara{End-to-end RL latency}
?? TODO
?? BIGGEST TO DO.

?? PCIe RTT is 10us (neugebauer, BNN NFP paper), NFP RTT is 18us (own measurements). EMEM Ring one-way is 120ns.

?? In the name of fairness, assume that rule installation uses same mechanism we recommend, but show how bad it can get? I.e. with rule installation costs etc.

?? Cziva~\parencite{DBLP:phd/ethos/Cziva18} discusses vNF times.

\fakepara{Co-existence with the dataplane}
We were able to saturate the \SI{40}{\giga\bit\per\second} link for packet sizes at or above \SI{256}{\byte}.
%For frame sizes of \SIlist{64;128;256}{\byte} input traffic rates were \SIlist{17.4;32.9;37.0}{\giga\bit\per\second} respectively (\SIlist[per-symbol=p,sticky-per=true]{33.9;32.2;18.1}{\mega\packet\per\second}).
For frame sizes of \SIlist{64;128}{\byte} input traffic rates were \SIlist{17.4;32.9}{\giga\bit\per\second} respectively (\SIlist[per-symbol=p,sticky-per=true]{33.9;32.2}{\mega\packet\per\second}).
Passing this traffic over the loaded NFP device running \approachshort, no packet losses were incurred at any rate of RL actions.

We show the effect of RL workloads on the round-trip latencies of cross traffic through \cref{fig:dataplane-coop}.
As observed latency measures do not obey a normal distribution (particularly \SIlist{256;1518}{\byte}, which are bimodal), we employ a one-tailed \emph{Mann-Whitney U test} to mark statistically significant population increases in latency ($p < 0.05$) with a ``+''.
In general, we observe that statistically significant latency increases concentrate around smaller packet sizes.
All (bar one) of these affected \nth{99} percentile latencies by under \SI{0.38}{\percent} (at most \SI{78}{\nano\second}).
This slight general degradation can be explained by increased pressure on the NFP's \emph{command push-pull} (CPP) bus, which is responsible for handling cross-island accesses to memory (particularly IMEM/EMEM) and other resources.
\approachshort{} places load on the CPP bus through use of its \inring{}/\outring{} rings for external communication and through last-tier policy accesses.
This also explains the sensitivity of \SI{256}{\byte} packets to \approachshort{}---the Netronome P4 toolchain segments packets, storing metadata (e.g., MAC prepend data) and the first bytes of a packet in a $\SI{256}{\byte}$ CTM block and parking the bodies of processed packets in EMEM.
\SI{256}{\byte} packets overshoot this due to metadata, causing small I/O accesses at a high rate for packets sized around this cutoff point.
Regardless, this effect is small when observed.

The anomalous result is \SI{128}{\byte} packets at \num{3000} RL action/update computations per second, causing a \SI{222}{\nano\second} (\SI{1.18}{\percent}) increase, shown in \cref{fig:dataplane-example}.
This is observed through a shift of some packet latencies from the mode towards the tail, but no other changes in the distribution.
In the above context, we believe that the inbound request rate is weakly synchronised with inbound packets, causing a higher level of burstiness around accesses to the CPP bus.
We expect that dedicated hardware/FPGA designs can avoid this problem by having dedicated \inring{}/\outring{} access mechanisms for an \approachshort{} agent.

\begin{figure}
	\centering
	\begin{subfigure}{0.45\linewidth}
		\resizebox{1.0\linewidth}{!}{
%			\includegraphics{../plots/build/stress/heat-latency-median}
			\includegraphics{../plots/build/stress/heat-latency-two-9-rel}
%			\includegraphics{../plots/build/stress/heat-latency-two-9-perc}
		}
		\caption{Deviations in \nth{99} percentile cross-traffic RTTs.\label{fig:dataplane-heat}}
	\end{subfigure}
	\hspace{0.05\linewidth}
		\begin{subfigure}{0.45\linewidth}
		\resizebox{1.0\linewidth}{!}{
			\includegraphics{../plots/build/stress/histo-128B-0-3-trim}
		}
		\caption{Distribution of RTTs for \SI{128}{\byte} packets for baseline and \num{3000} RL actions/s.\label{fig:dataplane-example}}
	\end{subfigure}
	\caption{Effects on tail latency of cross-traffic caused by different loads of off-path RL compute. Statistically significant increases in population latency are concentrated on smaller packet sizes, and are typically sub-\SI{78}{\nano\second}.\label{fig:dataplane-coop}}
\end{figure}

\fakepara{Resource requirements}
\Cref{tab:resources} demonstrates how \approachshort{} consumes shared and local memory as it scales to fit a device's compute resources, compared with a simple P4 forwarding application.
As one program is installed per ME, these results represent the minimum and maximum resource use we can command on a single island (i.e., without replacing P4 workers).
We observe negligible costs on shared bulk EMEM storage ($\sim$\SI{4}{\mebi\byte}), including storage space for a hash-table of \num{4096} state-action pairs and \num{16} reward values.
The most significant costs arise due to policy data (\SI{405}{\kibi\byte} capacity shared IMEM, \SI{90}{\kibi\byte} local CTM, \SI{15}{\kibi\byte} local CLS), which can be halved or quartered using \SIlist{16;8}{\bit} quantisation and remain constant regardless of compute unit usage.
Thread local storage in CLS for compute/register spilling scales with the number of MEs required (requiring \SIlist{13.77;15.49}{\percent} per-ME for \Indfw{}/\Coopfw) due to precaching and the space needed to hold tile lists.
Due to the initial policy cost this falls below the fair share at 3 MEs, but always allows for co-existence with other asynchronous dataplane programs.

\begin{table}
\caption{NFP memory use due to \approachshort{} using \numlist{1;4} MEs (\SI{32}{\bit}). CLS and CTM regions are shared between all programs on the same island (placing our RL agent on i5), while EMEM and IMEM regions are shared between all NFP programs on a NIC.\label{tab:resources}}
\resizebox{\linewidth}{!}{
\begin{tabular}{
		@{}c
		S[table-format=4.2] S[table-format=2.2]
		S[table-format=4.2] S[table-format=2.2]
		S[table-format=4.2] S[table-format=2.2]
		S[table-format=2.2] S[table-format=2.2]
		S[table-format=3.2] S[table-format=2.2]
		@{}
	}
	\toprule Firmware & \multicolumn{2}{c}{EMEM} & \multicolumn{2}{c}{EMEM Cache} & \multicolumn{2}{c}{IMEM} & \multicolumn{2}{c}{i5.CLS} & \multicolumn{2}{c}{i5.CTM}\\
	& \multicolumn{1}{c}{\si{\mebi\byte}} & \multicolumn{1}{c}{\si{\percent}} & \multicolumn{1}{c}{\si{\kibi\byte}} & \multicolumn{1}{c}{\si{\percent}} & \multicolumn{1}{c}{\si{\kibi\byte}} & \multicolumn{1}{c}{\si{\percent}} & \multicolumn{1}{c}{\si{\kibi\byte}} & \multicolumn{1}{c}{\si{\percent}} & \multicolumn{1}{c}{\si{\kibi\byte}} & \multicolumn{1}{c}{\si{\percent}} \\
	\midrule Base P4 & 6776.67 & 88.24 & 268.52 & 2.91 & 858.28 & 10.48 & 0.00 & 0.00 & 0.00 & 0.00 \\
	\Indfw(1) & 6780.21 & 88.28 & 2541.08 & 27.57 & 1263.28 & 15.42 & 24.75 & 38.67 & 94.25 & 36.82 \\
	\Indfw(4) & 6780.22 & 88.28 & 2545.33 & 27.62 & 1263.28 & 15.42 & 51.18 & 79.97 & 107.00 & 41.80 \\
	\Coopfw(1) & 6779.12 & 88.27 & 1773.59 & 19.24 & 1263.28 & 15.42 & 22.41 & 35.01 & 90.00 & 35.16 \\
	\Coopfw(4) & 6779.12 & 88.27 & 1769.84 & 19.20 & 1263.28 & 15.42 & 52.16 & 81.49 & 90.00 & 35.16 \\
	\bottomrule
\end{tabular}
}
\end{table}

%\fakepara{The impact of bit depth}
%?? \Cref{fig:quant-acc}
%?? \SI{5}{\bit} mantissa suffices for $\ge$ \SI{90}{\percent} relative accuracy, so \SI{8}{\bit} is fine (1S + 2E + 5M), \SI{16}{\bit} is better.
%
%\begin{figure}
%	\includegraphics{../plots/build/marl-quant/accuracy-binary}
%	\caption{Normalised accuracy of a converted, pre-trained floating-point tile-coded policy after conversion to $32Qn$ fixed-point.}\label{fig:quant-acc}
%\end{figure}

\fakepara{Deployability}
We found that setup and tiling packets take a mean \SIlist{27.03;16.69}{\micro\second} to be processed on \Indfw{}, allowing for an agent to be swapped between online and offline operation (or repurposed for another task/policy) painlessly.
Tiling packet processing was found to scale linearly with the number of \emph{tiling sets}, due to the needed precaching of tile set boundaries.
Online-offline swaps for \Coopfw{} exhibit similar cost, however the need for explicit ahead-of-time heuristic scheduling means that policy/tiling \emph{structure} changes (including the first complete setup) take \SI{422.63}{\micro\second} for the full-size policy described above.
The time taken for \Coopfw{} to schedule its tasks was found to scale with the number of workers ($m$) and work items ($n$) as described above ($\mathcal{O}{\left(n\log{m}\right)}$).
Ignoring the trivial solutions, reducing the worker count to \num{1} costs a mean \SI{238.22}{\micro\second}, while placing a single task incurs \SI{53.79}{\micro\second}.
Policy \emph{data} changes require no additional work in any case, resolving purely to \texttt{memcpy}s.

We observed that firmware installation (i.e., changing from \Indfw{} to \Coopfw{}, bit depth, or increasing maximum policy sizes) took a mean time of \SI{38.83}{\second}.
In the event that appropriate firmwares are not pre-compiled, we found that compiling and linking both \approachshort{} and the P4 toolchain took around \SI{35}{\second}, while changing only \approachshort{} parameters required around \SI{25}{\second}.
These results show that \approachshort{} can be easily adapted and altered by network administrators once in place, and illustrates an advantage of SoC-based SmartNICs.

\section{Potential Integrations}\label{sec:potential-integrations}
To show the general applicability and use of \approachshort{}, we propose an ideal integrations which would benefit from in-NIC reinforcement learning; online DDoS attack mitigation.
We support this using other state-of-the-art P4/PDP developments, and discuss how best to balance the concerns of online training with throughput in widespread deployment.
Additionally, we discuss how network administrators may combine offline (\Indfw) and online (\Coopfw) agents within a network to achieve online learning while maximising action throughput in the majority of the network.

\subsection{In-Network DDoS Defence}\label{sec:integ-1}
Classical reinforcement learning has seen recent use in real-time, adaptive DDoS mitigation~\parencite{DBLP:journals/tnsm/SimpsonRP20}.
\Citeauthor{DBLP:journals/tnsm/SimpsonRP20}'s \emph{Guarded} agent design uses a mixture of global network state and local, per-flow state to monitor how flows respond to bandwidth changes and packet drop---applying the observations made by SPIFFY~\parencite{DBLP:conf/ndss/KangGS16} with more allowance for congestion-unaware protocols.
Actions then move flows up or down in punishment levels according to a finite state machine.

\fakepara{Why in-NIC?}
The above approach relies upon co-hosting traffic measurement and analysis alongside OpenFlow-compatible switches at the edge nodes of an AS.
However, packet mirroring, offloading RL computation to a host (potentially over layers of virtualisation), and CPU performance are all sources of additional, consistent state-action latency.
Both traffic statistics collection \emph{and} data-driven learning must be executed on such hosts/network functions.
Unless running these stages in a dedicated pipeline (adding further processing latency), resource contention between these processes will further impact tail latencies.
Naturally, this requires high-performance hardware to be stacked at network egress points, potentially beyond reasonable space, power, or ventilation constraints.
The solution to implement and improve upon this work using \approachshort{} is to place its RL agents on SmartNICs at AS edge nodes---a bump-in-the-wire deployment.

?? Any way to move some of this sooner? I.e., this is ``why move DDoS prevention here'', Stefanos suggested backporting some of this to justify ``why \emph{RL} in-NIC''?

\fakepara{Inputs}
To collate the required inputs and state, we then examine the recent innovations of the community.
Low-latency, pure-P4 solutions to extract and record per-flow TCP state directly in the dataplane such as Dapper~\parencite{DBLP:conf/sosr/GhasemiBR17} are well-studied.
In fact, the statistics offered by Dapper are a super-set of the local input state values employed by \emph{Guarded} agents, offering an opportunity to further improve their efficacy. 
We propose placing such monitors in the P4 dataplane, existing on-chip alongside the \approachshort{} agent.
The required global state (load measures from network paths) must still come from elsewhere in the network; this is now the element at highest risk of becoming stale, but the least likely to vary significantly in response to individual actions.

The original work suggests theoretical ``ground truth'' reward functions, whose correct implementation and designs were left as an open challenge. 
We posit that INDDoS~\parencite{tnms-ddos-victim-ident}, which uses Count-min Sketches to estimate DDoS victim cardinality, could be used as an effective reward function source---i.e., using the number of detected victims as a loss value.

\fakepara{Integrating \approachshort}
Before each Dapper monitoring action, we require that the table hosting this hybrid solution polls \approachshort{}'s \emph{OUT Ring} for any generated actions.
As we note in \cref{sec:limitations}, these actions would be placed into a small local hash table and simultaneously exported to the attached controller to be inserted as P4 rules in batches.
After the main monitoring step, packet ingress timestamps would be used to emulate the \emph{Timed Random Sequential} (TRS) scheduler used by the anti-DDoS agents for rate-controlled work, where state vectors would be selected and passed into \approachshort.
By design, active flows which are not \emph{judged} after a configurable time are discarded to prune the work set and allow new flows to be seen by the agent.
The tight bounds on execution time known \emph{a priori} make it easy to calculate the maximum number of decisions which can be made per deadline.
Reward values would then be separately inserted by a modified INDDoS table.

%?? Can I have a P4 code snippet here?

A reduction in state-action latency (i.e., with \Coopfw) is, as always, useful for minimising the noise inherent in learning.
However, an agent is limited by the fact that it can take at least one RTT for meaningful changes to occur in a flow's behaviour ($\mathcal{O}{\left(\si{\milli\second}\right)}$ in a transit AS/ISP).
Accordingly, this use-case benefits most from an increase in \emph{throughput} using \Indfw{}.
In this context, higher throughput means that network flows are more likely to be judged in every timestep, even when flow cardinality is high---making it more likely that changes in flow behaviour will be observed, acted on, and learned from.

We note that the TRS scheduler is designed to handle cases where the number of flows created by an attacker, combining state vectors and information over time while the asynchronous agent is itself busy.
An increased number of (attacking) flows beyond the maximum throughput simply makes it take longer in expectation for a flow to be reassessed.
As we shall examine in \Cref{sec:evaluation}, \approachshort{} far exceeds the action throughput of a host based deployment could handle.
The control plane is then free to use wildcards or specific matches to narrow down (or expand) the set of flows to be considered and controlled, though explicit scheduling techniques are still key in such adversarial environments.

%\subsection{Something host-specific?}\label{sec:integ-2}
%Accelerate something at an end-host... Flow control? Something datacentre-y? A novel CCA? What?

\subsection{Network Deployment Considerations}
The two compute models discussed above need not be homogeneously deployed.
For instance, in a networked deployment a subset of \approachshort{} nodes could be \SI{32}{\bit} \coopfw{} agents, training online, while most other nodes ran \SI{8}{\bit} \indfw{} designs to meet throughput guarantees.
The control plane would then be responsible for combining, downsampling, and distributing these improved policies between remaining agents.
This can be taken further still, using policy deltas or execution traces to enable out-of-path transfer learning for more complex function approximators such as neural networks.

\section{Related Work}

?? NOTES: try to strengthen emphasis on ``other works can't do online learning in the dataplane''. This is absolutely, \SI{100}{\percent} the killer feature.

?? A switch/NIC architecture like \textcite{DBLP:conf/hotnets/StephensAS18} would remove the need for hard-coded island-island relationships.

?? Event-driven model could better support many timer-driven RL use-cases \textcite{DBLP:conf/hotnets/IbanezABM19}.

?? Taurus. They can't train online though!~\parencite{DBLP:journals/corr/abs-2002-08987}

?? Do switches~\parencite{DBLP:conf/hotnets/XiongZ19}.
?? Train offline, classical methods targetting all match-action P4 systems (no externs etc).

\parencite{DBLP:journals/corr/abs-2002-08987,DBLP:journals/corr/abs-2009-02353,DBLP:conf/sigcomm/SanvitoSB18,DBLP:journals/corr/abs-1801-05731,langlet-ml-netronome}

?? N3IC run NNs on NIC~\parencite{DBLP:journals/corr/abs-2009-02353}, allow packet tagging to occur based on pre-trained BNN~\parencite{DBLP:conf/nips/HubaraCSEB16}.
?? both NFP (\SI{45}{\micro\second}) and NetFPGA (\SI{0.3}{\micro\second}).
?? We handle larger inputs (versus their \SI{256}{\bit}), offer online training.
?? Use our 5-dim as a point of comparison?

?? Or as an offload mechanism~\parencite{DBLP:conf/sigcomm/SanvitoSB18,DBLP:journals/corr/abs-1801-05731}.
?? Binary NNs ``more suitable for network hardware''~\parencite{DBLP:journals/corr/MiyashitaLM16}?

?? Can we collect some papers on per-packet ML/RL? Per-packet classification using NNs to build decision trees~\parencite{DBLP:conf/sigcomm/LiangZJS19}

?? RL for congestion control. ~\parencite{DBLP:journals/corr/abs-1910-04054}

?? Arbitrary training enhanced by in-network? General NNs~\parencite{DBLP:conf/micro/LiPAYQPWSEK18}, RL-specfic~\parencite{DBLP:conf/isca/LiLYCSH19}.

?? Mechanisms to support full in-switch DDoS detection~\cite{tnms-ddos-victim-ident}.

?? Simialr HW has been used to get NN \emph{inference} \textcite{langlet-ml-netronome}---although \emph{actual NN latency} in his Fig 4.4 (IPG is inter-packet gap: his is not truly async but achieves perf by lying on the packet path. Note that 0 IPG murders his latencies).

\section{Conclusion}

?? Future work: allow scaling over multiple islands.
?? Future work: NetFPGA it.

?? Future directions for community: want to see more general-purpose async compute on-chip! Larger form-factors!

?? Are there other ``old ways of doing things'' worth looking at?

\begin{acks}
	The authors would like to thank Rhys Simpson for his comments, and for discussions on the soundness of SIMD-like optimisations.
	They would additionally like to thank Stefanos Sagkriotis, Mircea Iordache-\c{S}ic\u{a}, and Haruna Adoga for their comments and feedback.
	This work was supported in part by the \grantsponsor{gs-epsrc}{Engineering and Physical Sciences Research Council}{https://epsrc.ukri.org/} [grant~\grantnum{gs-epsrc}{EP/N509668/1}].
\end{acks}
	
%\bibliographystyle{ACM-Reference-Format}
%\bibliography{reference}
\printbibliography

%\begin{appendices}
%	\section{Old ``What could we do''?}
%	How do we evaluate this without looking into classification performance?
%	\begin{itemize}
%		\item Analytically?
%		\begin{enumerate}
%			\item Prove that tile splitting scheme over memory hierarchy can improve performance.
%			\begin{itemize}
%				\item Hit rate of different tiles according to tile dimensionality?
%				\item Access time for each tier of memory hierarchy.
%			\end{itemize}
%		\end{enumerate}
%		\item Experimentally?
%		\begin{enumerate}
%			\item Create ``VNF'' testbed: mirror traffic from switch to a host for telemetry processing/flow state. Send flow state over to another node who computes RL actions. RL actions installed on switch.
%			\begin{itemize}
%				\item Why not co-host RL computation with telemetry? Could be separate functions, could want to ensure that neither impacts the performance of the other.
%			\end{itemize}
%			\item Create ``PDP'' testbed: Load firmware, pass in traffic. Traffic processing/action compute/table mod occurs in PDP.
%			\item What traffic? Healthy mix of attack (UDP), Discord legit Opus VoIP traffic, HTTP bulk-transfer traffic.
%			\item Measure $t_{1 \cdots 3}$ according to \cref{fig:state-slip} in both VNF and PDP contexts.
%			\begin{description}
%				\item[$t_1$]
%				\begin{description}
%					\item[PDP] Packet ingress $\rightarrow$ INT Island $\rightarrow$ State arrival in Control ME.
%					\item[VNF] Packet mirror time $\rightarrow$ telemetry VNF $\rightarrow$ state arrival in agent vnf.
%				\end{description}
%				
%				\item[$t_2$]
%				\begin{description}
%					\item[PDP] Time to compute action over parallel Policy MEs (RL Island, SmartNIC).
%					\item[VNF] Time to compute action in host program (on VNF) from received state.
%				\end{description}
%				
%				\item[$t_3$]
%				\begin{description}
%					\item[PDP] Modify local tables across ME/Island boundaries (same device), verify rule present (or get ACK).
%					\item[VNF] Generate OpenFlow control messages, send OF message to switch, verify rule present (or get ACK).
%				\end{description}
%			\end{description}
%			\item HYPOTHESIS: PDP will present a reduction in these three times for per-flow state.
%			\item QUESTION: Does same apply to learning?
%			\item NOTE: since we're not actually assessing the \emph{quality} of output decisions, we can cut corners on what data is sent out and where. I.e., in-band network telemetry island can be skipped, if we move over representative volumes of state to/from the RL island with approximately correct delay as recorded by other studies.
%		\end{enumerate}
%	\end{itemize}
%	I can think of one way to evaluate this if we employ classification performance:
%	\begin{enumerate}
%		\item Run the existing work~\parencite{DBLP:journals/tnsm/SimpsonRP20}, acquire a policy for one node after full info sharing.
%		\item Load this policy onto the RL Island (\cref{fig:netro-arch}).
%		\item Either feed in raw state packets, or feed in traffic if we can get INT working.
%		\item For each level of quantisation needed, examine average performance.
%		\item Don't compare against old numbers for final perf: use a ``VNF'' setup similar to above---this will probably suffer a bit compared to old results, where everything was on the same machine. Make it relative/normalised.
%	\end{enumerate}
%
%	\section{Intermediate steps?}
%	System?
%	\begin{itemize}
%		\item Setup other core as event source.
%		\begin{itemize}
%			\item \emph{\num{2} week.}
%			\item Idea: proxy for idea of INT core mentioned elsewhere, combined with the main periodic event sending that the last RL work relied upon. So, say "you can put that state management and extraction on another core, here's what the comms cost is" or similar.
%		\end{itemize}
%		\item Fixes to hash table use, signalling.
%		\begin{itemize}
%			\item \emph{\num{1} week.}
%			\item Blocks progress on optimisation, correct multi-stream RL.
%		\end{itemize}
%		\item Optimise/parallelise action compute/learning.
%		\begin{itemize}
%			\item \emph{\num{2} week.}
%			\item Value? Proves this approach can scale according to resource availability/demand on the deployment device, at compile-time anyhow.
%		\end{itemize}
%	\end{itemize}
%	
%	Experiments?
%	\begin{itemize}
%		\item Measure sum of times $t_{1...3}$ described elsewhere.
%		\begin{itemize}
%			\item \emph{\num{1} week.}
%			\item Currently have individual, need ``end-to-end''.
%		\end{itemize}
%		\item Throughput impact testing.
%		\begin{itemize}
%			\item \emph{\num{1} week.}
%			\item Requires current impl, creation of basic testbed and packet forwarding setup.
%			\item Use-case independent.
%		\end{itemize}
%		\item Measure execution, training costs for varying policy sizes.
%		\begin{itemize}
%			\item \emph{\num{1.5} weeks.}
%			\item Policies need not be meaningful, and can be garbage data. Use-case independent.
%			\item Partially blocked on multicore accelerated/optimised execution.
%		\end{itemize}
%		\item Measure rule installation cost on different platforms.
%		\begin{itemize}
%			\item \emph{\num{1.5} weeks.}
%			\item Blocked on creation of basic testbed and packet forwarding setup.
%		\end{itemize}
%		\item Investigate stationarity.
%		\begin{itemize}
%			\item \emph{\numrange{2}{3} weeks.}
%			\item Requires full use-case implementation.
%			\item Requires careful thought about sensible changes to normality.
%			\item Might be bolstered by comparison against an offline-trained system?
%		\end{itemize}
%	\end{itemize}
%	
%	\section{Everything we could possibly evaluate}
%	
%	Evaluation will need to be divided into 2 main categories:
%	\begin{enumerate}
%		\item Simulation/emulation based on mininet.
%		\begin{itemize}
%			\item These can only assess algorithmic properties, as performance measurements in mininet (i.e., a full mult-agent system) are not representative.
%		\end{itemize}
%		\item Testbed setup (2/3 configurations):
%		\begin{itemize}
%			\item These will assess key system performance properties of a \emph{single agent}, in control of a single switch/NIC as a bump in the wire. Bump-in-the-wire device will carry traffic between two hosts.
%			\item \emph{Time permitting, may need to send state vectors in packets rather than do feature/state extraction on-chip}.
%			\item \emph{Necessary.} RL runs on bump-in-the-wire SmartNIC.
%			\item \emph{Either/both.} P4-based off-path RL. Custom packet actions (e.g., random drop) will be implemented in P4 with externs. Packets/digests mirrored to state extraction machine. Agent may be co-hosted with state-extraction. Rules inserted via P4-device specific RTE.
%			\item \emph{Either/both.} OvS-based off-path RL. Custom packet actions are already implemented in OvS as OpenFlow extensions. Packets/digests mirrored to state extraction machine. Agent may be co-hosted with state-extraction.
%		\end{itemize}
%	\end{enumerate}
%	
%	\subsection{What is this different from, or better than?}
%	List below mainly for the benefit of helping enumerate everything I can compare against, and what their drawbacks are.
%	
%	\begin{itemize}
%		\item (Deep) neural network-backed RL.
%		\begin{itemize}
%			\item NNs infeasible to update online. Ordinarily this requires larger volumes of data than can be gathered in real time, but becomes substantially more difficult after change of representation (quantisation, binarisation). An online system (such as this) should be able to adapt to changes in underlying traffic, optimisation criteria, etc., dynamically.
%			\item Complexity of function approximation means that policy updates are more computationally complex to compute. For instance, NNs require use of the backpropagation algorithm, rather than the simpler classical schemes which need only the tile set already identified in action selection.
%			\item Only heavily quantised (i.e., binary neural networks) suitable for in-switch execution.
%		\end{itemize}
%		
%		\item MARL and other ``agents as network functions'' designs.
%		\begin{itemize}
%			\item Action, state collection, updates etc. happen on nodes outside of the main packet path in this paradigm. Thus, state measurements, action computation, and so on, should induce the state drift described above. Also makes these approaches ``slower to react'' in theory.
%		\end{itemize}
%	\end{itemize}
%	
%	At present, there are no other RL works in PDPs, let alone with online policy updates.
%	Other existing RL use cases suitable for adaptation to the ``bump-in-the-wire'' model are proving tricky to find.
%	
%	\subsection{Emulation metrics}
%	Key (MVP) metrics will be \textbf{bolded}, and expanded with a subsubsection.
%	
%	\textbf{Quantisation accuracy}, quantised training accuracy, \textbf{stationarity}.
%	
%	\subsubsection{Quantisation accuracy}
%	Examination of whether lower resolution quantisation impacts RL agent decision accuracy substantially.
%	
%	This has been done using MARL in mininet, see \cref{fig:quant-acc}.
%	
%	\subsubsection{Interaction with stationarity}
%	Learning time of the system as compared to known changes in normality.
%	
%	I.e., in the DDoS case: how long does an agent take to respond to the onset of an attack?
%	A change in attack strategy?
%	The conclusion of an attack?
%	There are no citations on \emph{how} DDoS attacks evolve, just that they do, and that this can occur on various timescales.
%	
%	\subsection{Testbed metrics}
%	Key (MVP) metrics will be \textbf{bolded}, and expanded with a subsubsection.
%	
%	\textbf{Traffic throughput (bytes, packets)}, \textbf{packet processing delay}, \textbf{action execution time}, \textbf{learning time}, from-scratch learning accuracy, \textbf{rule installation delay/cost}, host and NIC memory costs, policy installation time, ``accuracy'' in the DDoS case.
%	
%	\subsubsection{Traffic throughput}
%	Main research question: does moving RL logic onto the NIC reduce maximum throughput of the NIC, even if calculation is on a different functional unit?
%	This is relevant in both \si{\byte\per\second} and packets per second.
%	
%	This is application independent, and would assess whether the RL units (acting independently on the netronome) affect throughput compared with a basic P4 program at different work intensities.
%	Ideally, this would show no effect on forwarding performance.
%	
%	\subsubsection{Packet processing delay}
%	Main research question: does the presence of off-path (but on-chip) RL execution meaningfully increase packet processing delay? How does this compare to a P4 program with an equivalent number of tables? What about to OvS?
%	
%	\subsubsection{Action execution \& learning time}
%	Main research question: does moving RL logic from the host to the NIC substantially reduce delays from state measurement, to state receipt, to action computation and installation?
%	These constitute $t_{1...3}$ discussed above.
%	Primarily, these need to be looked at using varying policy sizes and distribution across memory regions.
%	
%	We ideally want these both with and without any acceleration from extra cores on the netronome: the point of this being that we can demonstrate that the system (or a similar NetFPGA system) can scale upwards according to available system resources or demands.
%	
%	\subsubsection{Rule installation delay/cost.}
%	Main research question: how long does it take to install rules in the above testbed setups? What is the downtime of so doing?
%	
%	This is application-dependent: see earlier discussions on rule batching and local extern/hash-table use.
%\end{appendices}

	
\end{document}